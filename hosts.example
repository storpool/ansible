# ansible hosts inventory file for StorPool deployment
# all variables can also be defined per node in the roles sections below
# NOTE: all boolean values must be declared as True/False (capital first letter only!)

[all:vars]
# Global variables for all roles

# Remote ssh variables for ansible
# NOTE: If user!=root /etc/sudoers on nodes needs to be adjusted accordingly
# ansible_user='storpool'	# defaults use ssh_config
# ansible_user='root'		# defaults use ssh_config
# ansible_port=22		# defaults use ssh_config

[storpool:vars]
# StorPool-specific variables

# sp_cluster = 'ansible-test' # This is the top directory where Ansible configuration is saved
# sp_custdir = '/home/cust' # The parent directory of sp_cluster
# sp_toolsdir = '/root/storpool'  # Path on the target hosts where StorPool files will be stored - Defaults to /root/storpool

# Where to get the StorPool release package from - Defaults to "web"
# sp_release_file = '/path/storpool-...'  # Local .tgz release file stored on ansible deployment host
# sp_release_file = 'web' # Download release from StorPool

# StorPool version to be deployed (if sp_release_file='web') - Defaults to 19.01
# sp_release = '19.01'

# StorPool target release - release, pre-release or sp-lab
# sp_target_release = 'release' # Default 

# Deployment type - Defaults to False
# sp_new_cluster = True	# New cluster deployment
# sp_new_cluster = False	# Adding node(s) to existing cluster (default)

# Path to storpool.conf on the ansible host - Defaults to the inventory directory (Where this file is located)
# sp_config = '/path/to/storpool.conf'

# Perform OS update (yum/apt update/upgrade) on target hosts - Defaults to True
# sp_update_system = True

# Set to True if deploying on a machine with single NIC
# sp_single_iface=False # Default

# Automatically configure network interfaces based on storpool.conf vars - Defaults to False
# sp_configure_network = False

# Use an alternative network configuration setup tool, used in internal testing - Defaults to False
# Implications: when configured, overrides the normal network configuration.
# sp_alternative_net_setup = False # Default

# Overwrite existing ineterface configuration files (iface-genconf -o) - Defaults to False
# sp_overwrite_iface_conf = False

# Set SELinux state (valid options are "permissive" or "disabled") - Defaults to disabled
# sp_selinux = 'disabled'

# stop and disable NetworkManager - Defaults to true
# sp_disable_nm = True

# Stop and disable firewalld/ufw (if False, will add ports for StorPool) - Defaults to True
# sp_disable_fw = True

# Node is a Virtual Machine - It will be determined automatically if not specifically set
# sp_vm = False

# Generate a summary and pause after variable validation - Defaults to True
# sp_summary_wait = True

# Drive selection section
#   - Values can be obtained by running 'lsblk -do NAME,SIZE,MODEL' on the nodes
#   - Devices matching _any_ of the conditions below will get initialized
#   - Larger NVME drives will be split into equally-sized partitions smaller than 4T
#   - A matched device, already containing partitions, will trigger an error
#     remove partitions manually or override with sp_drive_erase=True
#
# CAUTION: The drive_init procedure is not fully implemented yet
#   - No RAID controller detection, and private case scenarios
#   - Needs ability to specify storpool_initdisk parameters

# Coma-separated list of devices to initialize
# sp_drives='nvme0n1,nvme1n1,sdb,sdc'

# Coma-separated list of sizes of devices to initialize
# sp_drive_size='3.5T,1.8T,223.6G'

# Coma-separated list of models of devices to initialize
# sp_drive_model='ST2000NM003A,INTEL SSDPE2KX080T8,SAMSUNG MZ7LH3T8'

# Init the drives even if they contain partitions (use with caution) - Defaults to False
# NOTE: StorPool partitions will never be touched regardless of this flag
# sp_drive_erase=False

# Comma-separated list of roles for the node(s) being deployed
# NOTES:
#   - All roles will get '@block cli' services installed
#   - Defining 'server' and 'client' will treat the node as hyperconverged
#
# Available roles:
# client	# client hypervisor node
# server[:N]	# N - max instances (1 if omitted)
# iscsi
# mgmt
# bridge

# sp_node_roles='client'	# Default
# sp_node_roles='server,iscsi'	# Another Example

# List of arguments to pass to storpool_cg conf
# NOTE: See 'storpool_cg conf -h' for all available arguments
#
# Examples:
# sp_cg_conf_extra='CORES=2 NUMA_OVERFLOW=1 SP_COMMON_LIMIT=2G SP_ALLOC_LIMIT=2G'
# sp_cg_conf_extra='SYSTEM_LIMIT=4G USER_LIMIT=2G MACHINE_LIMIT=4G'
#
# Default if client+server is defined in sp_node_roles
# sp_cg_conf_extra='CONVERGED=1'
#
# Default otherwise
# sp_cg_conf_extra=''
#
# Offset to calculate diskid prefix (useful when sp_node_id >= 40)
# diskid_prefix = sp_node_id - sp_diskid_offset
# sp_diskid_offset=0	# Default

# Enable Self encrypting drives (SED)
# sp_sed_enabled=False # Default
#
# If sp_sed_universal_password is set, all devices will be encrypted with the specified password.
# Otherwise a random password will be set for each drive.
# sp_sed_universal_password=''
#
# Enable ATA Security Feature Set for SATA SED devices
# sp_sed_enable_atasf=False # Default
#
# Enable OPAL encrypting for NVMe SED devices
# sp_sed_enable_opal=False # Default
#
# Path to the SED configuration file
# sp_sed_conf='/etc/storpool/sed.conf'

[storpool]
# Node hostnames and per-node variables
# NOTE: If no variables are defined will use globals/defaults from above
#
# Examples:
# sp-node.1  sp_node_roles='server:7,mgmt' sp_drive_size='2T,7T' sp_drives='sdb,sdc'
# sp-node.2  sp_node_roles='server:7,mgmt' sp_drive_size='2T' sp_drive_model='SAMSUNG MZQLB7T6HMLA-00007'
# sp-node.3  sp_node_roles='server:3,mgmt' sp_drive_model='ST2000NM003A,INTEL SSDPE2KX080T8'
# sp-node.4  sp_node_roles='server:4' sp_drives='nvme0n1,nvme1n1,nvme2n1,nvme3n1'
# sp-node.5  sp_node_roles='server:4' sp_drive_size='4T'
# sp-node.6  sp_node_roles='server:2' sp_drives='nvme0n1,nvme1n1'
# sp-node.11 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
# sp-node.12 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
# sp-node.13 sp_node_roles='server,iscsi,bridge' sp_drives='sdb,sdc'
# sp-node.21 sp_node_roles='client' sp_disable_fw=False
# sp-node.22
# sp-node.23
# sp-node.31 sp_node_roles='bridge' sp_vm=True sp_configure_network=False
# sp-node.32 sp_vm=True sp_configure_network=False sp_disable_nm=False
